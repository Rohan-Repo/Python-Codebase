{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30c73d2d-4966-4086-a5c3-01f40fe0fe9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot use HTMLSession within an existing event loop. Use AsyncHTMLSession instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m     keyword \u001b[38;5;241m=\u001b[39m keyword_map[job_title]\n\u001b[0;32m     60\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?keywords=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeyword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&location=Canada&geoId=101174742&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 61\u001b[0m     job_data \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_jobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     all_job_data\u001b[38;5;241m.\u001b[39mextend(job_data)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Create a Pandas DataFrame\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m, in \u001b[0;36mscrape_jobs\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Send a GET request and render JavaScript content\u001b[39;00m\n\u001b[0;32m     10\u001b[0m response \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhtml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Create a BeautifulSoup object\u001b[39;00m\n\u001b[0;32m     14\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mhtml\u001b[38;5;241m.\u001b[39mhtml, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\requests_html.py:586\u001b[0m, in \u001b[0;36mHTML.render\u001b[1;34m(self, retries, script, wait, scrolldown, sleep, reload, timeout, keep_page)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, retries: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m, script: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, wait: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m, scrolldown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, sleep: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, reload: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, timeout: Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8.0\u001b[39m, keep_page: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    542\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reloads the response in Chromium, and replaces HTML content\u001b[39;00m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;124;03m    with an updated version, with JavaScript executed.\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;124;03m    Chromium into your home directory (``~/.pyppeteer``).\u001b[39;00m\n\u001b[0;32m    584\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 586\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbrowser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbrowser\u001b[49m  \u001b[38;5;66;03m# Automatically create a event loop and browser\u001b[39;00m\n\u001b[0;32m    587\u001b[0m     content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;66;03m# Automatically set Reload to False, if example URL is being used.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\requests_html.py:729\u001b[0m, in \u001b[0;36mHTMLSession.browser\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    727\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m--> 729\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use HTMLSession within an existing event loop. Use AsyncHTMLSession instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_browser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop\u001b[38;5;241m.\u001b[39mrun_until_complete(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mbrowser)\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_browser\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot use HTMLSession within an existing event loop. Use AsyncHTMLSession instead."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from requests_html import HTMLSession\n",
    "\n",
    "# Function to scrape job data\n",
    "def scrape_jobs(url):\n",
    "    session = HTMLSession()\n",
    "\n",
    "    # Send a GET request and render JavaScript content\n",
    "    response = session.get(url)\n",
    "    response.html.render()\n",
    "\n",
    "    # Create a BeautifulSoup object\n",
    "    soup = BeautifulSoup(response.html.html, 'html.parser')\n",
    "\n",
    "    # Find job listings\n",
    "    job_listings = soup.find_all('li', class_='result-card')\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    job_data = []\n",
    "\n",
    "    # Loop through job listings and extract data\n",
    "    for job_listing in job_listings:\n",
    "        job_title = job_listing.find('h3', class_='result-card__title').get_text(strip=True)\n",
    "        company_name = job_listing.find('h4', class_='result-card__subtitle').get_text(strip=True)\n",
    "        location = job_listing.find('span', class_='job-result-card__location').get_text(strip=True)\n",
    "        posted_date = job_listing.find('time')['datetime']\n",
    "        job_description = job_listing.find('p', class_='job-result-card__snippet').get_text(strip=True)\n",
    "        skills = [skill.get_text(strip=True) for skill in job_listing.find_all('li', class_='job-criteria__item')]\n",
    "\n",
    "        job_data.append({\n",
    "            'Job Title': job_title,\n",
    "            'Company Name': company_name,\n",
    "            'Location': location,\n",
    "            'Posted Date': posted_date,\n",
    "            'Job Description': job_description,\n",
    "            'Skills': skills\n",
    "        })\n",
    "\n",
    "    session.close()\n",
    "    return job_data\n",
    "\n",
    "# Set the base URL for the LinkedIn job search page\n",
    "base_url = 'https://ca.linkedin.com/jobs/search'\n",
    "\n",
    "# Specify job titles and their keywords\n",
    "job_titles = ['Data Analyst', 'Full Stack Developer', 'Python Developer']\n",
    "keyword_map = {\n",
    "    'Data Analyst': 'Data%20Analyst',\n",
    "    'Full Stack Developer': 'Full%20Stack%20Developer',\n",
    "    'Python Developer': 'Python%20Developer'\n",
    "}\n",
    "\n",
    "# Initialize a list to store all job data\n",
    "all_job_data = []\n",
    "\n",
    "# Scrape job data for specified job titles\n",
    "for job_title in job_titles:\n",
    "    keyword = keyword_map[job_title]\n",
    "    url = f'{base_url}?keywords={keyword}&location=Canada&geoId=101174742&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0'\n",
    "    job_data = scrape_jobs(url)\n",
    "    all_job_data.extend(job_data)\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame(all_job_data)\n",
    "\n",
    "# Summarize skills required for each job title\n",
    "skills_summary = df.groupby('Job Title')['Skills'].sum().apply(pd.Series).stack().value_counts()\n",
    "\n",
    "# Display the summarized skills\n",
    "print(\"Skills Summary:\")\n",
    "print(skills_summary)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('linkedin_jobs_data.csv', index=False)\n",
    "\n",
    "# df = pd.read_csv('linkedin_jobs_data.csv')\n",
    "\n",
    "# Print the first few rows of the DataFrame for reference\n",
    "print(\"Sample Job Data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Summarize skills required for each job title\n",
    "skills_summary = df.groupby('Job Title')['Skills'].sum().apply(pd.Series).stack().value_counts()\n",
    "\n",
    "# Display the summarized skills\n",
    "print(\"\\nSkills Summary:\")\n",
    "print(skills_summary)\n",
    "\n",
    "# Create a bar plot to visualize the top skills\n",
    "top_skills = skills_summary.head(10)  # You can adjust the number of top skills to display\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_skills.plot(kind='bar')\n",
    "plt.title('Top Skills Required for Different Job Titles')\n",
    "plt.xlabel('Skill')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a93ec-d553-4aa5-8209-d534e7dc547f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cf65d58-3e87-4993-ba2e-60e23313a415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abc\\AppData\\Local\\Temp\\ipykernel_10484\\1229376803.py:40: RuntimeWarning: coroutine 'AsyncHTMLSession.close' was never awaited\n",
      "  session.close()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Job Title'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 71\u001b[0m\n\u001b[0;32m     68\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(all_job_data)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Summarize skills required for each job title\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m skills_summary \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mJob Title\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSkills\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mapply(pd\u001b[38;5;241m.\u001b[39mSeries)\u001b[38;5;241m.\u001b[39mstack()\u001b[38;5;241m.\u001b[39mvalue_counts()\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Display the summarized skills\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkills Summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:8399\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[0;32m   8396\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   8397\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[1;32m-> 8399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   8400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8402\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8405\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8407\u001b[0m \u001b[43m    \u001b[49m\u001b[43msqueeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8410\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:959\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrouper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_grouper\n\u001b[1;32m--> 959\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmutated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmutated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m obj\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:888\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[0;32m    886\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m    891\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Job Title'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from requests_html import AsyncHTMLSession\n",
    "\n",
    "# Allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Async function to scrape job data\n",
    "async def scrape_jobs(url):\n",
    "    session = AsyncHTMLSession()\n",
    "\n",
    "    # Send a GET request and render JavaScript content\n",
    "    response = await session.get(url)\n",
    "    await response.html.arender()\n",
    "\n",
    "    # Create a BeautifulSoup object\n",
    "    soup = BeautifulSoup(response.html.html, 'html.parser')\n",
    "\n",
    "    # Find job listings\n",
    "    job_listings = soup.find_all('li', class_='result-card')\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    job_data = []\n",
    "\n",
    "    # Loop through job listings and extract data\n",
    "    for job_listing in job_listings:\n",
    "        # ... Extract job data using BeautifulSoup ...\n",
    "        job_data.append({\n",
    "            'Job Title': job_title,\n",
    "            'Company Name': company_name,\n",
    "            'Location': location,\n",
    "            'Posted Date': posted_date,\n",
    "            'Job Description': job_description,\n",
    "            'Skills': skills\n",
    "        })\n",
    "\n",
    "    session.close()\n",
    "    return job_data\n",
    "    \n",
    "# Set the base URL for the LinkedIn job search page\n",
    "base_url = 'https://ca.linkedin.com/jobs/search'\n",
    "\n",
    "# Specify job titles and their keywords\n",
    "job_titles = ['Data Analyst', 'Full Stack Developer', 'Python Developer']\n",
    "keyword_map = {\n",
    "    'Data Analyst': 'Data%20Analyst',\n",
    "    'Full Stack Developer': 'Full%20Stack%20Developer',\n",
    "    'Python Developer': 'Python%20Developer'\n",
    "}\n",
    "\n",
    "# Initialize a list to store all job data\n",
    "all_job_data = []\n",
    "\n",
    "# Use asyncio to run the asynchronous scraping function\n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "# Scrape job data for specified job titles\n",
    "for job_title in job_titles:\n",
    "    keyword = keyword_map[job_title]\n",
    "    url = f'{base_url}?keywords={keyword}&location=Canada&geoId=101174742&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0'\n",
    "    job_data = loop.run_until_complete(scrape_jobs(url))\n",
    "    all_job_data.extend(job_data)\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame(all_job_data)\n",
    "\n",
    "# Summarize skills required for each job title\n",
    "skills_summary = df.groupby('Job Title')['Skills'].sum().apply(pd.Series).stack().value_counts()\n",
    "\n",
    "# Display the summarized skills\n",
    "print(\"Skills Summary:\")\n",
    "print(skills_summary)\n",
    "\n",
    "# Create a bar plot to visualize the top skills\n",
    "top_skills = skills_summary.head(10)  # You can adjust the number of top skills to display\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_skills.plot(kind='bar')\n",
    "plt.title('Top Skills Required for Different Job Titles')\n",
    "plt.xlabel('Skill')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544bfef6-8ffd-4197-b6e4-d63097360e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e589dd3-cfba-4432-ac38-da44fc8a1d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\Lib\\site-packages\\pyppeteer\\util.py:29: RuntimeWarning: coroutine 'scrape_jobs' was never awaited\n",
      "  gc.collect()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\abc\\AppData\\Local\\Temp\\ipykernel_10484\\816045740.py:40: RuntimeWarning: coroutine 'AsyncHTMLSession.close' was never awaited\n",
      "  session.close()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Job Title'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 71\u001b[0m\n\u001b[0;32m     68\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(all_job_data)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Summarize skills required for each job title\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m skills_summary \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mJob Title\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSkills\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mapply(pd\u001b[38;5;241m.\u001b[39mSeries)\u001b[38;5;241m.\u001b[39mstack()\u001b[38;5;241m.\u001b[39mvalue_counts()\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Display the summarized skills\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkills Summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:8399\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[0;32m   8396\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   8397\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[1;32m-> 8399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   8400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8402\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8405\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8407\u001b[0m \u001b[43m    \u001b[49m\u001b[43msqueeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8410\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:959\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrouper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_grouper\n\u001b[1;32m--> 959\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmutated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmutated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m obj\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:888\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[0;32m    886\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m    891\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Job Title'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from requests_html import AsyncHTMLSession\n",
    "\n",
    "# Allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Async function to scrape job data\n",
    "async def scrape_jobs(url):\n",
    "    session = AsyncHTMLSession()\n",
    "\n",
    "    # Send a GET request and render JavaScript content\n",
    "    response = await session.get(url)\n",
    "    await response.html.arender()\n",
    "\n",
    "    # Create a BeautifulSoup object\n",
    "    soup = BeautifulSoup(response.html.html, 'html.parser')\n",
    "\n",
    "    # Find job listings\n",
    "    job_listings = soup.find_all('li', class_='result-card')\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    job_data = []\n",
    "\n",
    "    # Loop through job listings and extract data\n",
    "    for job_listing in job_listings:\n",
    "        # ... Extract job data using BeautifulSoup ...\n",
    "        job_data.append({\n",
    "            'Job Title': job_title,\n",
    "            'Company Name': company_name,\n",
    "            'Location': location,\n",
    "            'Posted Date': posted_date,\n",
    "            'Job Description': job_description,\n",
    "            'Skills': skills\n",
    "        })\n",
    "\n",
    "    session.close()\n",
    "    return job_data\n",
    "\n",
    "# Set the base URL for the LinkedIn job search page\n",
    "base_url = 'https://ca.linkedin.com/jobs/search'\n",
    "\n",
    "# Specify job titles and their keywords\n",
    "job_titles = ['Data Analyst', 'Full Stack Developer', 'Python Developer']\n",
    "keyword_map = {\n",
    "    'Data Analyst': 'Data%20Analyst',\n",
    "    'Full Stack Developer': 'Full%20Stack%20Developer',\n",
    "    'Python Developer': 'Python%20Developer'\n",
    "}\n",
    "\n",
    "# Initialize a list to store all job data\n",
    "all_job_data = []\n",
    "\n",
    "# Use asyncio to run the asynchronous scraping function\n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "# Scrape job data for specified job titles\n",
    "for job_title in job_titles:\n",
    "    keyword = keyword_map[job_title]\n",
    "    url = f'{base_url}?keywords={keyword}&location=Canada&geoId=101174742&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0'\n",
    "    job_data = loop.run_until_complete(scrape_jobs(url))\n",
    "    all_job_data.extend(job_data)\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame(all_job_data)\n",
    "\n",
    "# Summarize skills required for each job title\n",
    "skills_summary = df.groupby('Job Title')['Skills'].sum().apply(pd.Series).stack().value_counts()\n",
    "\n",
    "# Display the summarized skills\n",
    "print(\"Skills Summary:\")\n",
    "print(skills_summary)\n",
    "\n",
    "# Create a bar plot to visualize the top skills\n",
    "top_skills = skills_summary.head(10)  # You can adjust the number of top skills to display\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_skills.plot(kind='bar')\n",
    "plt.title('Top Skills Required for Different Job Titles')\n",
    "plt.xlabel('Skill')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6914496-da92-4e33-b692-93eade7cf6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2965c04-f38c-4d29-bece-686ad72d2144",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Job Title'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 85\u001b[0m\n\u001b[0;32m     82\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(all_job_data)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Summarize skills required for each job title\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m skills_summary \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mJob Title\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSkills\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mapply(pd\u001b[38;5;241m.\u001b[39mSeries)\u001b[38;5;241m.\u001b[39mstack()\u001b[38;5;241m.\u001b[39mvalue_counts()\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Display the summarized skills\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkills Summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:8399\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[0;32m   8396\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   8397\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[1;32m-> 8399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   8400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8402\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8405\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8407\u001b[0m \u001b[43m    \u001b[49m\u001b[43msqueeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8410\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:959\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrouper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_grouper\n\u001b[1;32m--> 959\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmutated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmutated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m obj\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:888\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[0;32m    886\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m    891\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Job Title'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from requests_html import AsyncHTMLSession\n",
    "\n",
    "# Allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Async function to scrape job data\n",
    "async def scrape_jobs(url):\n",
    "    session = AsyncHTMLSession()\n",
    "\n",
    "    # Send a GET request and render JavaScript content\n",
    "    response = await session.get(url)\n",
    "    await response.html.arender()\n",
    "\n",
    "    # Create a BeautifulSoup object\n",
    "    soup = BeautifulSoup(response.html.html, 'html.parser')\n",
    "\n",
    "    # Find job listings\n",
    "    job_listings = soup.find_all('li', class_='result-card')\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    job_data = []\n",
    "\n",
    "    # Loop through job listings and extract data\n",
    "    for job_listing in job_listings:\n",
    "        job_title_elem = job_listing.find('h3', class_='result-card__title')\n",
    "        company_name_elem = job_listing.find('h4', class_='result-card__subtitle')\n",
    "        location_elem = job_listing.find('span', class_='job-result-card__location')\n",
    "        posted_date_elem = job_listing.find('time')\n",
    "        job_description_elem = job_listing.find('p', class_='job-result-card__snippet')\n",
    "        skill_elements = job_listing.find_all('span', class_='job-criteria__text')\n",
    "\n",
    "        if job_title_elem and company_name_elem and location_elem and posted_date_elem and job_description_elem and skill_elements:\n",
    "            job_title = job_title_elem.get_text(strip=True)\n",
    "            company_name = company_name_elem.get_text(strip=True)\n",
    "            location = location_elem.get_text(strip=True)\n",
    "            posted_date = posted_date_elem['datetime']\n",
    "            job_description = job_description_elem.get_text(strip=True)\n",
    "            skills = [skill_elem.get_text(strip=True) for skill_elem in skill_elements]\n",
    "\n",
    "            job_data.append({\n",
    "                'Job Title': job_title,\n",
    "                'Company Name': company_name,\n",
    "                'Location': location,\n",
    "                'Posted Date': posted_date,\n",
    "                'Job Description': job_description,\n",
    "                'Skills': skills\n",
    "            })\n",
    "\n",
    "    await session.close()\n",
    "    return job_data\n",
    "\n",
    "# Set the base URL for the LinkedIn job search page\n",
    "base_url = 'https://ca.linkedin.com/jobs/search'\n",
    "\n",
    "# Specify job titles and their keywords\n",
    "job_titles = ['Data Analyst', 'Full Stack Developer', 'Python Developer']\n",
    "keyword_map = {\n",
    "    'Data Analyst': 'Data%20Analyst',\n",
    "    'Full Stack Developer': 'Full%20Stack%20Developer',\n",
    "    'Python Developer': 'Python%20Developer'\n",
    "}\n",
    "\n",
    "# Initialize a list to store all job data\n",
    "all_job_data = []\n",
    "\n",
    "# Use asyncio to run the asynchronous scraping function\n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "# Scrape job data for specified job titles\n",
    "for job_title in job_titles:\n",
    "    keyword = keyword_map[job_title]\n",
    "    url = f'{base_url}?keywords={keyword}&location=Canada&geoId=101174742&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0'\n",
    "    job_data = loop.run_until_complete(scrape_jobs(url))\n",
    "    all_job_data.extend(job_data)\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame(all_job_data)\n",
    "\n",
    "# Summarize skills required for each job title\n",
    "skills_summary = df.groupby('Job Title')['Skills'].sum().apply(pd.Series).stack().value_counts()\n",
    "\n",
    "# Display the summarized skills\n",
    "print(\"Skills Summary:\")\n",
    "print(skills_summary)\n",
    "\n",
    "# Create a bar plot to visualize the top skills\n",
    "top_skills = skills_summary.head(10)  # You can adjust the number of top skills to display\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_skills.plot(kind='bar')\n",
    "plt.title('Top Skills Required for Different Job Titles')\n",
    "plt.xlabel('Skill')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeaecb4-8398-41e3-b10d-ec41de766bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9ecd15a-4332-4687-9d25-119a1026f33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from requests_html import AsyncHTMLSession\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Async function to scrape job data\n",
    "async def scrape_jobs(url):\n",
    "    session = AsyncHTMLSession()\n",
    "\n",
    "    # Send a GET request and render JavaScript content\n",
    "    response = await session.get(url)\n",
    "    await response.html.arender()\n",
    "\n",
    "    # Create a BeautifulSoup object\n",
    "    soup = BeautifulSoup(response.html.html, 'html.parser')\n",
    "\n",
    "    # Find job listings\n",
    "    job_listings = soup.find_all('li', class_='result-card')\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    job_data = []\n",
    "\n",
    "    # Loop through job listings and extract data\n",
    "    for job_listing in job_listings:\n",
    "        job_title_elem = job_listing.find('h3', class_='result-card__title')\n",
    "        company_name_elem = job_listing.find('h4', class_='result-card__subtitle')\n",
    "        location_elem = job_listing.find('span', class_='job-result-card__location')\n",
    "        posted_date_elem = job_listing.find('time')\n",
    "        job_description_elem = job_listing.find('p', class_='job-result-card__snippet')\n",
    "        skill_elements = job_listing.find_all('span', class_='job-criteria__text')\n",
    "\n",
    "        if job_title_elem and company_name_elem and location_elem and posted_date_elem and job_description_elem and skill_elements:\n",
    "            job_title = job_title_elem.get_text(strip=True)\n",
    "            company_name = company_name_elem.get_text(strip=True)\n",
    "            location = location_elem.get_text(strip=True)\n",
    "            posted_date = posted_date_elem['datetime']\n",
    "            job_description = job_description_elem.get_text(strip=True)\n",
    "            skills = [skill_elem.get_text(strip=True) for skill_elem in skill_elements]\n",
    "\n",
    "            job_data.append({\n",
    "                'Job Title': job_title,\n",
    "                'Company Name': company_name,\n",
    "                'Location': location,\n",
    "                'Posted Date': posted_date,\n",
    "                'Job Description': job_description,\n",
    "                'Skills': skills\n",
    "            })\n",
    "\n",
    "    await session.close()\n",
    "    return job_data\n",
    "\n",
    "# Set the base URL for the LinkedIn job search page\n",
    "base_url = 'https://ca.linkedin.com/jobs/search'\n",
    "\n",
    "# Specify job titles and their keywords\n",
    "job_titles = ['Data Analyst', 'Full Stack Developer', 'Python Developer']\n",
    "keyword_map = {\n",
    "    'Data Analyst': 'Data%20Analyst',\n",
    "    'Full Stack Developer': 'Full%20Stack%20Developer',\n",
    "    'Python Developer': 'Python%20Developer'\n",
    "}\n",
    "\n",
    "# Initialize a list to store all job data\n",
    "all_job_data = []\n",
    "\n",
    "# Use asyncio to run the asynchronous scraping function\n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "# Scrape job data for specified job titles\n",
    "for job_title in job_titles:\n",
    "    keyword = keyword_map[job_title]\n",
    "    url = f'{base_url}?keywords={keyword}&location=Canada&geoId=101174742&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0'\n",
    "    job_data = loop.run_until_complete(scrape_jobs(url))\n",
    "    all_job_data.extend(job_data)\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame(all_job_data)\n",
    "\n",
    "print(df)\n",
    "\n",
    "# Summarize skills required for each job title\n",
    "# skills_summary = df.groupby('Job Title')['Skills'].sum().apply(pd.Series).stack().value_counts()\n",
    "\n",
    "# # Display the summarized skills\n",
    "# print(\"Skills Summary:\")\n",
    "# print(skills_summary)\n",
    "\n",
    "# # Create a bar plot to visualize the top skills\n",
    "# top_skills = skills_summary.head(10)  # You can adjust the number of top skills to display\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# top_skills.plot(kind='bar')\n",
    "# plt.title('Top Skills Required for Different Job Titles')\n",
    "# plt.xlabel('Skill')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b986ac-d611-4783-965d-c6c0b65e7862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffc2bcad-315e-4f04-96cd-130ce4e0334c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Job Title'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 81\u001b[0m\n\u001b[0;32m     78\u001b[0m df\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Summarize skills required for each job title\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m skills_summary \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mJob Title\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSkills\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Display the summarized skills\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkills Summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:8399\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[0;32m   8396\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   8397\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[1;32m-> 8399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   8400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8402\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8405\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8407\u001b[0m \u001b[43m    \u001b[49m\u001b[43msqueeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8410\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:959\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrouper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_grouper\n\u001b[1;32m--> 959\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmutated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmutated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m obj\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:888\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[0;32m    886\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m    891\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Job Title'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from requests_html import AsyncHTMLSession\n",
    "\n",
    "# Allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Async function to scrape job data\n",
    "async def scrape_jobs(url):\n",
    "    session = AsyncHTMLSession()\n",
    "\n",
    "    # Send a GET request and render JavaScript content\n",
    "    response = await session.get(url)\n",
    "    await response.html.arender()\n",
    "\n",
    "    # Create a BeautifulSoup object\n",
    "    soup = BeautifulSoup(response.html.html, 'html.parser')\n",
    "\n",
    "    # Find job listings\n",
    "    job_listings = soup.find_all('li', class_='result-card')\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    job_data = []\n",
    "\n",
    "    # Loop through job listings and extract data\n",
    "    for job_listing in job_listings:\n",
    "        job_title_elem = job_listing.find('h3', class_='result-card__title')\n",
    "        company_name_elem = job_listing.find('h4', class_='result-card__subtitle')\n",
    "        location_elem = job_listing.find('span', class_='job-result-card__location')\n",
    "        skills_elem = job_listing.find('div', class_='job-result-card__meta')\n",
    "\n",
    "        if job_title_elem and company_name_elem and location_elem and skills_elem:\n",
    "            job_title = job_title_elem.get_text(strip=True)\n",
    "            company_name = company_name_elem.get_text(strip=True)\n",
    "            location = location_elem.get_text(strip=True)\n",
    "            skills = skills_elem.get_text(strip=True)\n",
    "\n",
    "            job_data.append({\n",
    "                'Job Title': job_title,\n",
    "                'Company Name': company_name,\n",
    "                'Location': location,\n",
    "                'Skills': skills\n",
    "            })\n",
    "\n",
    "    await session.close()\n",
    "    return job_data\n",
    "\n",
    "# Set the base URL for the LinkedIn job search page\n",
    "base_url = 'https://ca.linkedin.com/jobs/search?keywords=Data%20Analyst&location=Canada&geoId=&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0'\n",
    "\n",
    "# Specify job titles and their keywords\n",
    "job_titles = ['Data Analyst', 'Full Stack Developer', 'Python Developer']\n",
    "keyword_map = {\n",
    "    'Data Analyst': 'Data%20Analyst',\n",
    "    'Full Stack Developer': 'Full%20Stack%20Developer',\n",
    "    'Python Developer': 'Python%20Developer'\n",
    "}\n",
    "\n",
    "# Initialize a list to store all job data\n",
    "all_job_data = []\n",
    "\n",
    "# Use asyncio to run the asynchronous scraping function\n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "# Scrape job data for specified job titles\n",
    "for job_title in job_titles:\n",
    "    keyword = keyword_map[job_title]\n",
    "    url = f'{base_url}?keywords={keyword}&location=Canada&geoId=&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0'\n",
    "    job_data = loop.run_until_complete(scrape_jobs(url))\n",
    "    all_job_data.extend(job_data)\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame(all_job_data)\n",
    "\n",
    "df\n",
    "\n",
    "# Summarize skills required for each job title\n",
    "skills_summary = df.groupby('Job Title')['Skills'].sum()\n",
    "\n",
    "# Display the summarized skills\n",
    "print(\"Skills Summary:\")\n",
    "print(skills_summary)\n",
    "\n",
    "# Create a bar plot to visualize the skills\n",
    "plt.figure(figsize=(10, 6))\n",
    "skills_summary.plot(kind='bar')\n",
    "plt.title('Skills Required for Different Job Titles')\n",
    "plt.xlabel('Job Title')\n",
    "plt.ylabel('Skills')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195ead88-efc2-4cc5-bb84-fb51023fe187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb734f6c-bfd6-461c-82cd-0e27bc93e34d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NetworkError",
     "evalue": "Execution context was destroyed, most likely because of a navigation.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNetworkError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pyppeteer\\execution_context.py:99\u001b[0m, in \u001b[0;36mExecutionContext.evaluateHandle\u001b[1;34m(self, pageFunction, force_expr, *args)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 99\u001b[0m     _obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRuntime.callFunctionOn\u001b[39m\u001b[38;5;124m'\u001b[39m, {\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunctionDeclaration\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpageFunction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msuffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexecutionContextId\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_contextId,\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marguments\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convertArgument(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args],\n\u001b[0;32m    103\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturnByValue\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mawaitPromise\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    105\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muserGesture\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    106\u001b[0m     })\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\asyncio\\futures.py:287\u001b[0m, in \u001b[0;36mFuture.__await__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asyncio_future_blocking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\asyncio\\tasks.py:339\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 339\u001b[0m     \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "\u001b[1;31mNetworkError\u001b[0m: Protocol error (Runtime.callFunctionOn): Cannot find context with specified id",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNetworkError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Use asyncio to run the asynchronous scraping function\u001b[39;00m\n\u001b[0;32m     51\u001b[0m loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\n\u001b[1;32m---> 52\u001b[0m job_data \u001b[38;5;241m=\u001b[39m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscrape_jobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Create a Pandas DataFrame\u001b[39;00m\n\u001b[0;32m     55\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(job_data)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\nest_asyncio.py:99\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\asyncio\\tasks.py:269\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    267\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 269\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_must_cancel:\n\u001b[0;32m    272\u001b[0m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 16\u001b[0m, in \u001b[0;36mscrape_jobs\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Send a GET request and render JavaScript content\u001b[39;00m\n\u001b[0;32m     15\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m session\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m response\u001b[38;5;241m.\u001b[39mhtml\u001b[38;5;241m.\u001b[39marender()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Create a BeautifulSoup object\u001b[39;00m\n\u001b[0;32m     19\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mhtml\u001b[38;5;241m.\u001b[39mhtml, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\requests_html.py:626\u001b[0m, in \u001b[0;36mHTML.arender\u001b[1;34m(self, retries, script, wait, scrolldown, sleep, reload, timeout, keep_page)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m content:\n\u001b[0;32m    624\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 626\u001b[0m         content, result, page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_async_render(url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl, script\u001b[38;5;241m=\u001b[39mscript, sleep\u001b[38;5;241m=\u001b[39msleep, wait\u001b[38;5;241m=\u001b[39mwait, content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhtml, reload\u001b[38;5;241m=\u001b[39mreload, scrolldown\u001b[38;5;241m=\u001b[39mscrolldown, timeout\u001b[38;5;241m=\u001b[39mtimeout, keep_page\u001b[38;5;241m=\u001b[39mkeep_page)\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\requests_html.py:531\u001b[0m, in \u001b[0;36mHTML._async_render\u001b[1;34m(self, url, script, scrolldown, sleep, wait, reload, content, timeout, keep_page)\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m page\u001b[38;5;241m.\u001b[39m_keyboard\u001b[38;5;241m.\u001b[39mup(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPageDown\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    530\u001b[0m \u001b[38;5;66;03m# Return the content of the page, JavaScript evaluated.\u001b[39;00m\n\u001b[1;32m--> 531\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m page\u001b[38;5;241m.\u001b[39mcontent()\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m keep_page:\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m page\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pyppeteer\\page.py:761\u001b[0m, in \u001b[0;36mPage.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    760\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PageError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo main frame.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 761\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m frame\u001b[38;5;241m.\u001b[39mcontent()\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pyppeteer\\frame_manager.py:384\u001b[0m, in \u001b[0;36mFrame.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontent\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    383\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"Get the whole HTML contents of the page.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 384\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;124m() => \u001b[39m\u001b[38;5;124m{\u001b[39m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;124m  let retVal = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;124m  if (document.doctype)\u001b[39m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;124m    retVal = new XMLSerializer().serializeToString(document.doctype);\u001b[39m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;124m  if (document.documentElement)\u001b[39m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;124m    retVal += document.documentElement.outerHTML;\u001b[39m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;124m  return retVal;\u001b[39m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;124m}\u001b[39m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m'''\u001b[39m\u001b[38;5;241m.\u001b[39mstrip())\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pyppeteer\\frame_manager.py:308\u001b[0m, in \u001b[0;36mFrame.evaluate\u001b[1;34m(self, pageFunction, force_expr, *args)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ElementHandleError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExecutionContext is None.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m context\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[0;32m    309\u001b[0m     pageFunction, \u001b[38;5;241m*\u001b[39margs, force_expr\u001b[38;5;241m=\u001b[39mforce_expr)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pyppeteer\\execution_context.py:53\u001b[0m, in \u001b[0;36mExecutionContext.evaluate\u001b[1;34m(self, pageFunction, force_expr, *args)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, pageFunction: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any,\n\u001b[0;32m     48\u001b[0m                    force_expr: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Execute ``pageFunction`` on this context.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m    Details see :meth:`pyppeteer.page.Page.evaluate`.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluateHandle(\n\u001b[0;32m     54\u001b[0m         pageFunction, \u001b[38;5;241m*\u001b[39margs, force_expr\u001b[38;5;241m=\u001b[39mforce_expr)\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m handle\u001b[38;5;241m.\u001b[39mjsonValue()\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pyppeteer\\execution_context.py:108\u001b[0m, in \u001b[0;36mExecutionContext.evaluateHandle\u001b[1;34m(self, pageFunction, force_expr, *args)\u001b[0m\n\u001b[0;32m     99\u001b[0m     _obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRuntime.callFunctionOn\u001b[39m\u001b[38;5;124m'\u001b[39m, {\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunctionDeclaration\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpageFunction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msuffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexecutionContextId\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_contextId,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muserGesture\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    106\u001b[0m     })\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 108\u001b[0m     \u001b[43m_rewriteError\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m exceptionDetails \u001b[38;5;241m=\u001b[39m _obj\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexceptionDetails\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exceptionDetails:\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pyppeteer\\execution_context.py:237\u001b[0m, in \u001b[0;36m_rewriteError\u001b[1;34m(error)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot find context with specified id\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    236\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExecution context was destroyed, most likely because of a navigation.\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m--> 237\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(error)(msg)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "\u001b[1;31mNetworkError\u001b[0m: Execution context was destroyed, most likely because of a navigation."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from requests_html import AsyncHTMLSession\n",
    "\n",
    "# Allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def scrape_jobs(url):\n",
    "    session = AsyncHTMLSession()\n",
    "\n",
    "    # Send a GET request and render JavaScript content\n",
    "    response = await session.get(url)\n",
    "    await response.html.arender()\n",
    "\n",
    "    # Create a BeautifulSoup object\n",
    "    soup = BeautifulSoup(response.html.html, 'html.parser')\n",
    "\n",
    "    # Find job listings\n",
    "    job_listings = soup.find_all('li', class_='result-card')\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    job_data = []\n",
    "\n",
    "    # Loop through job listings and extract data\n",
    "    for job_listing in job_listings:\n",
    "        job_title_elem = job_listing.find('h3', class_='result-card__title')\n",
    "        company_name_elem = job_listing.find('h4', class_='result-card__subtitle')\n",
    "        skills_elem = job_listing.find_all('span', class_='job-criteria__text')\n",
    "\n",
    "        if job_title_elem and company_name_elem and skills_elem:\n",
    "            job_title = job_title_elem.get_text(strip=True)\n",
    "            company_name = company_name_elem.get_text(strip=True)\n",
    "            skills = [skill.get_text(strip=True) for skill in skills_elem]\n",
    "\n",
    "            job_data.append({\n",
    "                'Job Title': job_title,\n",
    "                'Company Name': company_name,\n",
    "                'Skills': skills\n",
    "            })\n",
    "\n",
    "    await session.close()\n",
    "    return job_data\n",
    "\n",
    "# URL of the LinkedIn job search page\n",
    "url = \"https://ca.linkedin.com/jobs/search?keywords=Data%20Analyst&location=Canada&geoId=&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0\"\n",
    "\n",
    "# Use asyncio to run the asynchronous scraping function\n",
    "loop = asyncio.get_event_loop()\n",
    "job_data = loop.run_until_complete(scrape_jobs(url))\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame(job_data)\n",
    "\n",
    "print(df)\n",
    "# # Create a list of all skills\n",
    "# all_skills = [skill for skills_list in df['Skills'] for skill in skills_list]\n",
    "\n",
    "# # Create a frequency distribution of skills\n",
    "# skill_counts = pd.Series(all_skills).value_counts()\n",
    "\n",
    "# # Plot the top skills\n",
    "# top_skills = skill_counts.head(10)\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# top_skills.plot(kind='bar')\n",
    "# plt.title('Top Skills Required for Data Analyst Jobs')\n",
    "# plt.xlabel('Skill')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# print(\"Skills Summary:\")\n",
    "# print(top_skills)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382725ab-232a-4a06-9df8-c2c77eb66403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d112bb88-0bae-46cb-ab53-1e33f733060e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abc\\AppData\\Local\\Temp\\ipykernel_10484\\3069414964.py:28: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  skills_counts = pd.Series(skills_list).value_counts()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skills Summary:\n",
      "Series([], dtype: int64)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m top_skills \u001b[38;5;241m=\u001b[39m skills_counts\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     36\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m---> 37\u001b[0m \u001b[43mtop_skills\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbar\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTop Skills Required for Data Analyst Jobs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     39\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSkill\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pandas\\plotting\\_core.py:1000\u001b[0m, in \u001b[0;36mPlotAccessor.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    997\u001b[0m             label_name \u001b[38;5;241m=\u001b[39m label_kw \u001b[38;5;129;01mor\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m    998\u001b[0m             data\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m label_name\n\u001b[1;32m-> 1000\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mplot_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pandas\\plotting\\_matplotlib\\__init__.py:71\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(data, kind, **kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124max\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(ax, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft_ax\u001b[39m\u001b[38;5;124m\"\u001b[39m, ax)\n\u001b[0;32m     70\u001b[0m plot_obj \u001b[38;5;241m=\u001b[39m PLOT_CLASSES[kind](data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 71\u001b[0m \u001b[43mplot_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m plot_obj\u001b[38;5;241m.\u001b[39mdraw()\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m plot_obj\u001b[38;5;241m.\u001b[39mresult\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:459\u001b[0m, in \u001b[0;36mMPLPlot.generate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_plot_logic_common(ax, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m--> 459\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post_plot_logic\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:1739\u001b[0m, in \u001b[0;36mBarPlot._post_plot_logic\u001b[1;34m(self, ax, data)\u001b[0m\n\u001b[0;32m   1736\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1737\u001b[0m     str_index \u001b[38;5;241m=\u001b[39m [pprint_thing(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])]\n\u001b[1;32m-> 1739\u001b[0m s_edge \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43max_pos\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.25\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlim_offset\n\u001b[0;32m   1740\u001b[0m e_edge \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39max_pos[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.25\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbar_width \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlim_offset\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decorate_ticks(ax, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_index_name(), str_index, s_edge, e_edge)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# URL of the LinkedIn job search page\n",
    "url = \"https://ca.linkedin.com/jobs/search?keywords=Data%20Analyst&location=Canada&geoId=&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0\"\n",
    "\n",
    "# Send a GET request\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find job listings\n",
    "listings = soup.find_all('li', class_='result-card')\n",
    "\n",
    "# Initialize a list to store skills\n",
    "skills_list = []\n",
    "\n",
    "# Extract job skills\n",
    "for listing in listings:\n",
    "    skills = listing.find_all('span', class_='job-criteria__text')\n",
    "    skills = [skill.get_text(strip=True) for skill in skills]\n",
    "    skills_list.extend(skills)\n",
    "\n",
    "# Create a frequency distribution of skills\n",
    "skills_counts = pd.Series(skills_list).value_counts()\n",
    "\n",
    "# Display the summarized skills\n",
    "print(\"Skills Summary:\")\n",
    "print(skills_counts)\n",
    "\n",
    "# Create a bar plot to visualize the top skills\n",
    "top_skills = skills_counts.head(10)\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_skills.plot(kind='bar')\n",
    "plt.title('Top Skills Required for Data Analyst Jobs')\n",
    "plt.xlabel('Skill')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3985054b-fc7b-4f6d-98fe-c3b88f72346b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
